---
layout: single
title: "VESSL_AI 강의"
use_math : True
categories: machine_learning
tags: machine_learning
toc: true
toc_sticky: true
---

## 0. 개요

"SNU GPU First with VESSL AI"   

학교에서 GPU대여 서비스를 제공하고 있고, 그 방법에 대한 강의 수강생을 모집하는 메일을 받아서 지원했다. 저번 학기 내가 가지고 있는 개인GPU가 없는 상황에서 졸업 프로젝트를 Google Colab을 이용해 진행했는데, 상당한 불편함을 느꼈다. 12시간마다 끊기는 런타임, 언제 끊길지 모르는 학습 등 무료로 제공하는 공유 GPU 서비스는 한계가 명확했다. 서울대학교에서 자교생에게 제공하는 무료 GPU대여 서비스를 이용한다면 앞으로의 개인 프로젝트와 공부를 보다 원활하게 할 수 있을 것 같아서 기대가 컸다.

## 1. MLOps

강의의 시작은 MLOps에 대한 설명이였다.

MLOps란 Machine Learning + DevOps로, 머신러닝 모델을 개발 및 배포, 운영하는 일련의 과정을 뜻한다. 

모델의 데이터 수집, 학습 모니터링, 데이터셋 버전 관리, 자동화 등 어떠한 문제에 대해 인공지능을 사용해 해결하고자 할 때 거치는 일련의 프로세스를 관리, 자동화하는 개념이다.

MLOps의 주요 특징은 아래와 같다.

1. DataOps
    데이터를 사용하기 좋게 라벨링/가공/피쳐 추출 및 적재

2. Experiment Tracking
    머신러닝 실험의 모든 기록과 결과물 저장/관리
    가장 기본적으로는 Epoch에 따른 Loss, Accuracy를 기록하는 것이 해당된다.
    주요 서비스: Tensorboard, Weights & Biases

3. Cluster Management & Job Orchestration
    머신러닝 실험을 돌리기 위한 클러스터와 클러스터에서 돌아가는 Training Job들을 관리, 운영하기 위한 기술
    Container 기반 Runtime을 실생할 수 있는 환경으로 주로 구축  
    주요 서비스: Ray, Kubeflow Job Scheduling
    GPU서버를 운영하는 측면에서 고려하는 부분이라고 이해했다.

4. Model Registry
    만들어진 모델을 과정과 결과와 함께 중앙화된 저장소에 저장
    주요 서비스: MLFlow-Registry

5. Model Serving
    만들어진 모델을 API 서비스화

6. ML Pipeline
    개발 및 배포 과정을 정의하는 작업
    주요 서비스: KubeFlow-pipeline

## 2. VESSL AI

강의의 2부는 VESSL AI를 이용해 SNU의 GPU서버에 접속, 사용하는 방법을 다루었다.

대략적인 사용법을 알게 되었는데, 자세한 방법은 직접 프로젝트 등을 하며 써보아야 알게 될 것 같다.

## 3. 느낀 점

강의를 들으면서, 내가 졸업 프로젝트를 진행할 떄 이 서비스를 알게 되었다면 큰 도움이 되었을 것이라는 생각을 했다.

결국 졸업 프로젝트에서 진행한 내용이 어떠한 문제를 ML을 이용해 해결하기 위한 일련의 프로세스를 구축하는 것인데, VESSL AI와 같은 상용화된 MLOps 서비스를 사용하였다면 체계화에 큰 도움을 받지 않았을까 생각한다. 또한 Google Colab같은 공유GPU를 이용하는 것보다 학교에서 제공하는 GPU서버를 이용하였다면 학습에 더 큰 도움이 되지 않았을까?

내 졸업 프로젝트는 어느정도 완성이 되었지만, 추후에 이러한 서비스를 이용하여 보다 재사용과 Fine-Tuning이 편하도록 프로젝트를 발전시켜볼까 생각이 든다.